{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_You are currently looking at **version 1.1** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-machine-learning/resources/bANLa) course resource._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4 - Understanding and Predicting Property Maintenance Fines\n",
    "\n",
    "This assignment is based on a data challenge from the Michigan Data Science Team ([MDST](http://midas.umich.edu/mdst/)). \n",
    "\n",
    "The Michigan Data Science Team ([MDST](http://midas.umich.edu/mdst/)) and the Michigan Student Symposium for Interdisciplinary Statistical Sciences ([MSSISS](https://sites.lsa.umich.edu/mssiss/)) have partnered with the City of Detroit to help solve one of the most pressing problems facing Detroit - blight. [Blight violations](http://www.detroitmi.gov/How-Do-I/Report/Blight-Complaint-FAQs) are issued by the city to individuals who allow their properties to remain in a deteriorated condition. Every year, the city of Detroit issues millions of dollars in fines to residents and every year, many of these fines remain unpaid. Enforcing unpaid blight fines is a costly and tedious process, so the city wants to know: how can we increase blight ticket compliance?\n",
    "\n",
    "The first step in answering this question is understanding when and why a resident might fail to comply with a blight ticket. This is where predictive modeling comes in. For this assignment, your task is to predict whether a given blight ticket will be paid on time.\n",
    "\n",
    "All data for this assignment has been provided to us through the [Detroit Open Data Portal](https://data.detroitmi.gov/). **Only the data already included in your Coursera directory can be used for training the model for this assignment.** Nonetheless, we encourage you to look into data from other Detroit datasets to help inform feature creation and model selection. We recommend taking a look at the following related datasets:\n",
    "\n",
    "* [Building Permits](https://data.detroitmi.gov/Property-Parcels/Building-Permits/xw2a-a7tf)\n",
    "* [Trades Permits](https://data.detroitmi.gov/Property-Parcels/Trades-Permits/635b-dsgv)\n",
    "* [Improve Detroit: Submitted Issues](https://data.detroitmi.gov/Government/Improve-Detroit-Submitted-Issues/fwz3-w3yn)\n",
    "* [DPD: Citizen Complaints](https://data.detroitmi.gov/Public-Safety/DPD-Citizen-Complaints-2016/kahe-efs3)\n",
    "* [Parcel Map](https://data.detroitmi.gov/Property-Parcels/Parcel-Map/fxkw-udwf)\n",
    "\n",
    "___\n",
    "\n",
    "We provide you with two data files for use in training and validating your models: train.csv and test.csv. Each row in these two files corresponds to a single blight ticket, and includes information about when, why, and to whom each ticket was issued. The target variable is compliance, which is True if the ticket was paid early, on time, or within one month of the hearing data, False if the ticket was paid after the hearing date or not at all, and Null if the violator was found not responsible. Compliance, as well as a handful of other variables that will not be available at test-time, are only included in train.csv.\n",
    "\n",
    "Note: All tickets where the violators were found not responsible are not considered during evaluation. They are included in the training set as an additional source of data for visualization, and to enable unsupervised and semi-supervised approaches. However, they are not included in the test set.\n",
    "\n",
    "<br>\n",
    "\n",
    "**File descriptions** (Use only this data for training your model!)\n",
    "\n",
    "    readonly/train.csv - the training set (all tickets issued 2004-2011)\n",
    "    readonly/test.csv - the test set (all tickets issued 2012-2016)\n",
    "    readonly/addresses.csv & readonly/latlons.csv - mapping from ticket id to addresses, and from addresses to lat/lon coordinates. \n",
    "     Note: misspelled addresses may be incorrectly geolocated.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Data fields**\n",
    "\n",
    "train.csv & test.csv\n",
    "\n",
    "    ticket_id - unique identifier for tickets\n",
    "    agency_name - Agency that issued the ticket\n",
    "    inspector_name - Name of inspector that issued the ticket\n",
    "    violator_name - Name of the person/organization that the ticket was issued to\n",
    "    violation_street_number, violation_street_name, violation_zip_code - Address where the violation occurred\n",
    "    mailing_address_str_number, mailing_address_str_name, city, state, zip_code, non_us_str_code, country - Mailing address of the violator\n",
    "    ticket_issued_date - Date and time the ticket was issued\n",
    "    hearing_date - Date and time the violator's hearing was scheduled\n",
    "    violation_code, violation_description - Type of violation\n",
    "    disposition - Judgment and judgement type\n",
    "    fine_amount - Violation fine amount, excluding fees\n",
    "    admin_fee - $20 fee assigned to responsible judgments\n",
    "state_fee - $10 fee assigned to responsible judgments\n",
    "    late_fee - 10% fee assigned to responsible judgments\n",
    "    discount_amount - discount applied, if any\n",
    "    clean_up_cost - DPW clean-up or graffiti removal cost\n",
    "    judgment_amount - Sum of all fines and fees\n",
    "    grafitti_status - Flag for graffiti violations\n",
    "    \n",
    "train.csv only\n",
    "\n",
    "    payment_amount - Amount paid, if any\n",
    "    payment_date - Date payment was made, if it was received\n",
    "    payment_status - Current payment status as of Feb 1 2017\n",
    "    balance_due - Fines and fees still owed\n",
    "    collection_status - Flag for payments in collections\n",
    "    compliance [target variable for prediction] \n",
    "     Null = Not responsible\n",
    "     0 = Responsible, non-compliant\n",
    "     1 = Responsible, compliant\n",
    "    compliance_detail - More information on why each ticket was marked compliant or non-compliant\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "Your predictions will be given as the probability that the corresponding blight ticket will be paid on time.\n",
    "\n",
    "The evaluation metric for this assignment is the Area Under the ROC Curve (AUC). \n",
    "\n",
    "Your grade will be based on the AUC score computed for your classifier. A model which with an AUROC of 0.7 passes this assignment, over 0.75 will recieve full points.\n",
    "___\n",
    "\n",
    "For this assignment, create a function that trains a model to predict blight ticket compliance in Detroit using `readonly/train.csv`. Using this model, return a series of length 61001 with the data being the probability that each corresponding ticket from `readonly/test.csv` will be paid, and the index being the ticket_id.\n",
    "\n",
    "Example:\n",
    "\n",
    "    ticket_id\n",
    "       284932    0.531842\n",
    "       285362    0.401958\n",
    "       285361    0.105928\n",
    "       285338    0.018572\n",
    "                 ...\n",
    "       376499    0.208567\n",
    "       376500    0.818759\n",
    "       369851    0.018528\n",
    "       Name: compliance, dtype: float32\n",
    "       \n",
    "### Hints\n",
    "\n",
    "* Make sure your code is working before submitting it to the autograder.\n",
    "\n",
    "* Print out your result to see whether there is anything weird (e.g., all probabilities are the same).\n",
    "\n",
    "* Generally the total runtime should be less than 10 mins. You should NOT use Neural Network related classifiers (e.g., MLPClassifier) in this question. \n",
    "\n",
    "* Try to avoid global variables. If you have other functions besides blight_model, you should move those functions inside the scope of blight_model.\n",
    "\n",
    "* Refer to the pinned threads in Week 4's discussion forum when there is something you could not figure it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Hyper Parameter Tuning\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Precision and auc - Imbalanced class\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We only take consider features in the test data columns so all common features are cosidered and rest are dropped & not considered \n",
    "2. From the Test Data : Some of features from initial consideration are as follows:\n",
    "    \n",
    "    Numerical Data:\n",
    "        1. Judgement amount ( Total Net Amount owed by person)\n",
    "        3. Late fee         (If any late fee incurred by the poi - person of interest)\n",
    "        4. Fine amount      (Original Amount)\n",
    "        P.S: All the fees like(state fees,admin fee etc are dropped cause its standard fee and does not help with prediction)\n",
    "    \n",
    "    Categorical Data:\n",
    "        This can have an impact on the prediction, as the person from a 'certain place' might not consider paying\n",
    "        and can have better understanding on prediction , atleast provide understanding if people are paying in certain   locations than others\n",
    "        \n",
    "        1. City\n",
    "        2. State\n",
    "        3. Pincode (Yet to decide)\n",
    "        4. Disposition\n",
    "        \n",
    "3.We try to choose features that dont blow up in to 100s and 1000s of features after creating dummy variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART 1: Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \n",
    "    train = pd.read_csv('train.csv', encoding=\"latin1\")\n",
    "    test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "    print('Length of Training Data ={}'.format(len(train)))\n",
    "    print('Length of Test Data ={}'.format(len(test)))\n",
    "    \n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART 2: Cleaning up the data\n",
    "        1. Removing unwanted rows (NA values) - Training Data ,  Test Data should not have any rows dropped.\n",
    "        2. Dropping unnecessary columns that doesnt help with predictions - For Both Test and Training Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaning_data(train,test):\n",
    "    \n",
    "    df = train.copy()\n",
    "    df_test = test.copy()\n",
    "    df = df[['ticket_id','state','zip_code','disposition','judgment_amount','late_fee','compliance']]\n",
    "    \n",
    "    # Among the features we intend to use only disposition ,judgement amout ,late fee(need to be converted to 0 and 1),compliance\n",
    "    \n",
    "    # Dropping NA values\n",
    "    df.dropna(axis = 0,inplace = True)\n",
    "    \n",
    "    # Converting compliance to int (0 and 1)\n",
    "    df['compliance'] = df['compliance'].astype(int)\n",
    "    \n",
    "    # Converting late fee in to binary (0 and 1)\n",
    "    df['late_fee']= df['late_fee'].apply(lambda x : 1 if x > 0 else 0 )\n",
    "    df_test['late_fee']= df_test['late_fee'].apply(lambda x : 1 if x > 0 else 0 )\n",
    "    \n",
    "    \n",
    "    # Keeping ticket id for final series result\n",
    "    df_test = df_test[['ticket_id','disposition','judgment_amount','late_fee']]\n",
    "    \n",
    "    return df,df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART 3: Creating Dummy variables (if required) and hot encoding categorical data\n",
    "        1. We need to hot encode the disposition (it becomes numbered )\n",
    "        2. Dont forget to do the same transformation in Test data\n",
    "            - We are replacing disposition in to reflect both the Training and Test data\n",
    "        3. Ziping up can make it easier to lookup that information (if you want)\n",
    "        \n",
    "  We are going to consider the following columns for our classifier\n",
    "     \n",
    "     \n",
    "     FOR X:\n",
    "     \n",
    "     1.(Disposition columns) - after hot encoding -4 columns\n",
    "     2. Judgement Amount\n",
    "     3. Late fee\n",
    "     \n",
    "     FOR Y:\n",
    "     1. Compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dummies(df,df_test):\n",
    "    import pandas as pd\n",
    "    the_replacement = {'Responsible (Fine Waived) by Deter':'Fine Waived','Responsible (Fine Waived) by Admis':'Fine Waived',\n",
    "                   'Responsible - Compl/Adj by Default':'Responsible by Default','Responsible - Compl/Adj by Determi':'Responsible by Determination',\n",
    "                  'Responsible by Dismissal':'Fine Waived'}\n",
    "    \n",
    "    # Training data\n",
    "    df.replace(the_replacement,inplace =True)\n",
    "    \n",
    "    # Test Data\n",
    "    df_test.replace(the_replacement,inplace =True)\n",
    "    \n",
    "    \n",
    "    # Creating Dummies for Training and Test Data\n",
    "    # Training\n",
    "    dummies = pd.get_dummies(df['disposition'])\n",
    "    df2 = pd.concat([df, dummies],axis =1)\n",
    "    #Test\n",
    "    dummies = pd.get_dummies(df_test['disposition'])\n",
    "    df2_test = pd.concat([df_test, dummies],axis =1)\n",
    "    df2_test.drop(['disposition'],axis =1,inplace =True)\n",
    "    \n",
    "    # We get X and y (target) from training dataframe\n",
    "    # We get the X and y from the cleaned dataframe\n",
    "    X = df2.drop(['ticket_id','state','zip_code','disposition','compliance'],axis =1)\n",
    "    y = df2['compliance']\n",
    "    \n",
    "    \n",
    "    \n",
    "    return X,y,df2_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART 4 : Choosing a Classifier and testing the performace of data (HyperParameter Tuning)\n",
    "    1. Logistic Regression\n",
    "    2. LinearSVM\n",
    "    3. Decision Tree\n",
    "    4. Random Forest\n",
    "    5. Naive Bayes\n",
    "    \n",
    "    Finding out the best model with the best parameter to give best performace\n",
    "    \n",
    "    I chose Logistic Regression and calculate auc score, best parameter and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2881: DtypeWarning: Columns (11,12,31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Training Data =250306\n",
      "Length of Test Data =61001\n",
      "Test set AUC: 0.774573115664\n",
      "Best_parameter: {'C': 0.1}\n",
      "Best score: 0.779510310335452\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ticket_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>284932</th>\n",
       "      <td>0.037996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285362</th>\n",
       "      <td>0.025589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285361</th>\n",
       "      <td>0.039906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285338</th>\n",
       "      <td>0.037996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285346</th>\n",
       "      <td>0.039906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285345</th>\n",
       "      <td>0.037996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285347</th>\n",
       "      <td>0.040895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285342</th>\n",
       "      <td>0.860059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285530</th>\n",
       "      <td>0.025589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284989</th>\n",
       "      <td>0.032781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285344</th>\n",
       "      <td>0.040895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285343</th>\n",
       "      <td>0.025589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285340</th>\n",
       "      <td>0.025589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285341</th>\n",
       "      <td>0.040895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285349</th>\n",
       "      <td>0.039906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285348</th>\n",
       "      <td>0.037996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284991</th>\n",
       "      <td>0.032781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285532</th>\n",
       "      <td>0.032781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285406</th>\n",
       "      <td>0.032781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285001</th>\n",
       "      <td>0.032781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285006</th>\n",
       "      <td>0.025589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285405</th>\n",
       "      <td>0.025589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285337</th>\n",
       "      <td>0.032781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285496</th>\n",
       "      <td>0.040895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285497</th>\n",
       "      <td>0.037996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285378</th>\n",
       "      <td>0.025589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285589</th>\n",
       "      <td>0.032781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285585</th>\n",
       "      <td>0.037996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285501</th>\n",
       "      <td>0.039906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285581</th>\n",
       "      <td>0.025589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376367</th>\n",
       "      <td>0.028969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376366</th>\n",
       "      <td>0.037075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376362</th>\n",
       "      <td>0.183615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376363</th>\n",
       "      <td>0.187470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376365</th>\n",
       "      <td>0.028969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376364</th>\n",
       "      <td>0.037075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376228</th>\n",
       "      <td>0.037075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376265</th>\n",
       "      <td>0.037075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376286</th>\n",
       "      <td>0.287000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376320</th>\n",
       "      <td>0.037075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376314</th>\n",
       "      <td>0.037075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376327</th>\n",
       "      <td>0.287000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376385</th>\n",
       "      <td>0.287000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376435</th>\n",
       "      <td>0.826453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376370</th>\n",
       "      <td>0.857245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376434</th>\n",
       "      <td>0.040895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376459</th>\n",
       "      <td>0.039906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376478</th>\n",
       "      <td>0.003401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376473</th>\n",
       "      <td>0.037075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376484</th>\n",
       "      <td>0.036175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376482</th>\n",
       "      <td>0.032781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376480</th>\n",
       "      <td>0.032781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376479</th>\n",
       "      <td>0.032781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376481</th>\n",
       "      <td>0.032781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376483</th>\n",
       "      <td>0.037075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376496</th>\n",
       "      <td>0.025589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376497</th>\n",
       "      <td>0.025589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376499</th>\n",
       "      <td>0.039906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376500</th>\n",
       "      <td>0.039906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369851</th>\n",
       "      <td>0.530052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61001 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "ticket_id          \n",
       "284932     0.037996\n",
       "285362     0.025589\n",
       "285361     0.039906\n",
       "285338     0.037996\n",
       "285346     0.039906\n",
       "285345     0.037996\n",
       "285347     0.040895\n",
       "285342     0.860059\n",
       "285530     0.025589\n",
       "284989     0.032781\n",
       "285344     0.040895\n",
       "285343     0.025589\n",
       "285340     0.025589\n",
       "285341     0.040895\n",
       "285349     0.039906\n",
       "285348     0.037996\n",
       "284991     0.032781\n",
       "285532     0.032781\n",
       "285406     0.032781\n",
       "285001     0.032781\n",
       "285006     0.025589\n",
       "285405     0.025589\n",
       "285337     0.032781\n",
       "285496     0.040895\n",
       "285497     0.037996\n",
       "285378     0.025589\n",
       "285589     0.032781\n",
       "285585     0.037996\n",
       "285501     0.039906\n",
       "285581     0.025589\n",
       "...             ...\n",
       "376367     0.028969\n",
       "376366     0.037075\n",
       "376362     0.183615\n",
       "376363     0.187470\n",
       "376365     0.028969\n",
       "376364     0.037075\n",
       "376228     0.037075\n",
       "376265     0.037075\n",
       "376286     0.287000\n",
       "376320     0.037075\n",
       "376314     0.037075\n",
       "376327     0.287000\n",
       "376385     0.287000\n",
       "376435     0.826453\n",
       "376370     0.857245\n",
       "376434     0.040895\n",
       "376459     0.039906\n",
       "376478     0.003401\n",
       "376473     0.037075\n",
       "376484     0.036175\n",
       "376482     0.032781\n",
       "376480     0.032781\n",
       "376479     0.032781\n",
       "376481     0.032781\n",
       "376483     0.037075\n",
       "376496     0.025589\n",
       "376497     0.025589\n",
       "376499     0.039906\n",
       "376500     0.039906\n",
       "369851     0.530052\n",
       "\n",
       "[61001 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def blight_model():\n",
    "    \n",
    "    train,test = load_data()\n",
    "    df,df_test = cleaning_data(train,test)\n",
    "    X,y,df2_test = dummies(df,df_test)\n",
    "    \n",
    "    \n",
    "    # Logistic Regression\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,random_state =0)\n",
    "    LogR = LogisticRegression()\n",
    "    parameters = {'C':[0.1,1,100]}\n",
    "\n",
    "    # Create a classifier object with the classifier and parameter candidates\n",
    "    clf = GridSearchCV(LogR,param_grid = parameters,scoring ='roc_auc')\n",
    "    clf.fit(X_train,y_train)\n",
    "\n",
    "    predicted_log = clf.decision_function(X_test)\n",
    "\n",
    "\n",
    "    print('Test set AUC:',roc_auc_score(y_test,predicted_log))\n",
    "    print('Best_parameter:',clf.best_params_)\n",
    "    print('Best score:',clf.best_score_)\n",
    "    \n",
    "    ticket_id = df2_test['ticket_id'].tolist()\n",
    "    df2_test.drop(['ticket_id'],axis =1,inplace =True)\n",
    "    \n",
    "    prob_ticket = clf.predict_proba(df2_test)\n",
    "    data = pd.Series(prob_ticket[:,1])\n",
    "    result = pd.DataFrame(data)\n",
    "    result['ticket_id'] =ticket_id\n",
    "    result = result.set_index('ticket_id')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return result\n",
    "\n",
    "blight_model()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-machine-learning",
   "graded_item_id": "nNS8l",
   "launcher_item_id": "yWWk7",
   "part_id": "w8BSS"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
